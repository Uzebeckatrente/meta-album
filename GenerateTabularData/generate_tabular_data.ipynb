{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import argparse\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import datetime\n",
    "import copy\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularErrors:\n",
    "    def FileError(self,file):\n",
    "        raise IOError('[-] File Not Found : '+file)\n",
    "        exit\n",
    "    def DirectoryError(self,directory):\n",
    "        raise IOError('[-] Directory Not Found : '+directory)\n",
    "        exit\n",
    "    def ColumnError(self,column):\n",
    "        raise ValueError('[-] Column Not Found : '+column)\n",
    "        exit\n",
    "err = TabularErrors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='padding:5px 5px 5px 5px; margin:15px 0px 0px 0px; background-color:red;'></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path of the dataset which contains images, labels.csv and info.json\n",
    "DATASET_PATH = './data_set'\n",
    "\n",
    "\n",
    "# Path of the directory where the tabular data will bee saved\n",
    "TABULAR_PATH = \"./data_set_tabular\"\n",
    "\n",
    "\n",
    "# Normalize the input images according to the way neural networks were pretrained on ImageNet\n",
    "USE_NORMALIZATION = False\n",
    "\n",
    "\n",
    "# To retrain the network on your data before generating tabular data\n",
    "RETRAIN = False\n",
    "\n",
    "# The path to directory where the training images are stored (128x128x3)\n",
    "RETRAIN_DATASET_PATH = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='padding:5px 5px 5px 5px; margin:15px 0px 0px 0px; background-color:red;'></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do not change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed for generating super-categories by the same random combination of categories\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path of the json.info file\n",
    "JSON_PATH = os.path.join(DATASET_PATH, \"info.json\")\n",
    "\n",
    "# Path of the CSV file which contains the label and image name\n",
    "CSV_PATH = os.path.join(DATASET_PATH, \"labels.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Tabular Dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(TABULAR_PATH):\n",
    "    os.makedirs(TABULAR_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Directories and Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Directory\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    err.DirectoryError(DATASET_PATH)\n",
    "    \n",
    "#Check JSON file\n",
    "if not os.path.isfile(JSON_PATH):\n",
    "    err.FileError(JSON_PATH)\n",
    "\n",
    "# Check CSV File\n",
    "if not os.path.isfile(CSV_PATH):\n",
    "    err.FileError(CSV_PATH)\n",
    "\n",
    "# Check Tabular Directory\n",
    "if not os.path.exists(TABULAR_PATH):\n",
    "    err.DirectoryError(TABULAR_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Retrain Directories and Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RETRAIN:\n",
    "    # Path of the json.info file\n",
    "    RETRAIN_JSON_PATH = os.path.join(RETRAIN_DATASET_PATH, \"info.json\")\n",
    "\n",
    "    # Path of the CSV file which contains the label and image name\n",
    "    RETRAIN_CSV_PATH = os.path.join(RETRAIN_DATASET_PATH, \"labels.csv\")\n",
    "\n",
    "\n",
    "    # Dataset Directory\n",
    "    if not os.path.exists(RETRAIN_DATASET_PATH):\n",
    "        err.DirectoryError(RETRAIN_DATASET_PATH)\n",
    "        \n",
    "    #Check JSON file\n",
    "    if not os.path.isfile(RETRAIN_JSON_PATH):\n",
    "        err.FileError(RETRAIN_JSON_PATH)\n",
    "\n",
    "    # Check CSV File\n",
    "    if not os.path.isfile(RETRAIN_CSV_PATH):\n",
    "        err.FileError(RETRAIN_CSV_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open (JSON_PATH, \"r\")\n",
    "info = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Retrain JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrain_info = None\n",
    "if RETRAIN:\n",
    "    f = open (RETRAIN_JSON_PATH, \"r\")\n",
    "    retrain_info = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True if CSV is tab separated otherwise false\n",
    "CSV_WITH_TAB = info[\"csv_with_tab\"]\n",
    "\n",
    "\n",
    "\n",
    "# Path of the directory where images to be used in this experiement are saved\n",
    "if info[\"images_in_sub_folder\"]:\n",
    "    IMAGE_PATH = os.path.join(DATASET_PATH, \"images\")\n",
    "else:  \n",
    "    IMAGE_PATH = DATASET_PATH\n",
    "\n",
    "    \n",
    "\n",
    "# category column name in csv\n",
    "CATEGORY_COLUMN = info[\"category_column_name\"]\n",
    "\n",
    "# image column name in csv\n",
    "IMAGE_COLUMN = info[\"image_column_name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings from Retrain JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RETRAIN:\n",
    "    RETRAIN_CSV_WITH_TAB = retrain_info[\"csv_with_tab\"]\n",
    "    \n",
    "    if retrain_info[\"images_in_sub_folder\"]:\n",
    "        RETRAIN_IMAGE_PATH = os.path.join(RETRAIN_DATASET_PATH, \"images\")\n",
    "    else:  \n",
    "        RETRAIN_IMAGE_PATH = RETRAIN_DATASET_PATH\n",
    "        \n",
    "        \n",
    "    # category column name in csv\n",
    "    RETRAIN_CATEGORY_COLUMN = retrain_info[\"category_column_name\"]\n",
    "\n",
    "    # image column name in csv\n",
    "    RETRAIN_IMAGE_COLUMN = retrain_info[\"image_column_name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "if CSV_WITH_TAB:\n",
    "    data = pd.read_csv(CSV_PATH, sep=\"\\t\", encoding=\"utf-8\") \n",
    "else:\n",
    "    data = pd.read_csv(CSV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data Shape : \", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Retrain CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RETRAIN:\n",
    "    \n",
    "    if RETRAIN_CSV_WITH_TAB:\n",
    "        retrain_data = pd.read_csv(RETRAIN_CSV_PATH, sep=\"\\t\", encoding=\"utf-8\") \n",
    "    else:\n",
    "        retrain_data = pd.read_csv(RETRAIN_CSV_PATH)\n",
    "    print(\"Data Shape : \", retrain_data.shape)\n",
    "    \n",
    "    print(retrain_data.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(csv_data, cat_col, img_col):\n",
    "    dictt = {}\n",
    "  \n",
    "    \n",
    "    csv_data['label_cat'] = csv_data[cat_col].astype('category')\n",
    "\n",
    "    dictt['categories'] = csv_data['label_cat'].cat.categories.values\n",
    "    dictt['images'] = csv_data[cat_col].value_counts().values\n",
    "\n",
    "    dictt['labels'] = csv_data[cat_col].values\n",
    "    dictt['labels_num'] =  csv_data['label_cat'].cat.codes.values\n",
    "    dictt['data'] = csv_data[img_col].values\n",
    "\n",
    "    \n",
    "    return dictt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_data = prepare_data(data, CATEGORY_COLUMN, IMAGE_COLUMN)\n",
    "\n",
    "retrain_prepared_data = None\n",
    "if RETRAIN:\n",
    "    retrain_prepared_data = prepare_data(retrain_data, RETRAIN_CATEGORY_COLUMN, RETRAIN_IMAGE_COLUMN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImgDataset(Dataset):\n",
    "    def __init__(self, dataset_images, dataset_labels, transform):\n",
    "\n",
    "        # Transforms\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.images = dataset_images\n",
    "        self.labels = dataset_labels\n",
    "        \n",
    "        self.data_len = len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        \n",
    "        single_img = self.images[index]\n",
    "        img_transformed = torch.from_numpy(single_img).long()\n",
    "        img_transformed = img_transformed.permute(2, 0, 1)\n",
    "        img_transformed = torch.from_numpy(np.array(img_transformed)).float() / 255.\n",
    "\n",
    "        single_label = self.labels[index]\n",
    "        single_label = single_label.astype(np.compat.long)\n",
    "        \n",
    "        return img_transformed, single_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(data_set, batch_size=64, shuffle=False):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    if not USE_NORMALIZATION:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "    else:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    \n",
    "    train_ds=ImgDataset(data_set['images'],data_set['labels_num'], transform)\n",
    "    \n",
    "    \n",
    "    \n",
    "  \n",
    "    print(\"Data: \", len(train_ds))\n",
    "   \n",
    "    \n",
    "    data_stats = {\n",
    "        \"train_images\" : len(train_ds)\n",
    "    }\n",
    "    \n",
    "    \n",
    "    dataloaders = {\n",
    "        'train':DataLoader(\n",
    "            train_ds, \n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "        )\n",
    "    }\n",
    "\n",
    "    dataset_sizes = {\n",
    "        'train':len(train_ds)\n",
    "    }\n",
    "    \n",
    "    return dataloaders, dataset_sizes, data_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel():\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    model = nn.Sequential(*list(model.children())[:-1])\n",
    "    model.fc = nn.Sequential(nn.Flatten())\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if name.startswith(\"layer4.1\"):\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = getModel()\n",
    "# x = torch.randn(64, 3, 128, 128)\n",
    "# out = m(x)\n",
    "# out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, dataset_sizes, criterion, optimizer, scheduler, num_epochs=5):\n",
    "    \n",
    "    \n",
    "    print(\"--------------------------------------------\")\n",
    "    print(\"Training\")\n",
    "    print(\"--------------------------------------------\")\n",
    "    \n",
    "    since = time.time()\n",
    "\n",
    "\n",
    "    print(\"Epoch: \", end=\" \")\n",
    "    for epoch in range(num_epochs):\n",
    "        print(epoch, end=\" \")\n",
    "        \n",
    "\n",
    "        # Each epoch has a training phase\n",
    "\n",
    "        model.train()\n",
    "\n",
    "\n",
    "        # Iterate over data.\n",
    "        for inputs, labels in dataloaders['train']:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            # track history if only in train\n",
    "            with torch.set_grad_enabled(True):\n",
    "\n",
    "                outputs = model(inputs)\n",
    "#                 probabilities = F.softmax(outputs, dim=1)\n",
    "#                 _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "\n",
    "                # backward + optimize\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "\n",
    "           \n",
    "\n",
    "        # end dataloader loop\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    training_time = '{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60)\n",
    "    \n",
    "    print('Training complete in: {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    print()\n",
    "\n",
    "\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, dataloaders, dataset_sizes):\n",
    "    \n",
    "    \n",
    "    print(\"--------------------------------------------\")\n",
    "    print(\"Extracting Predictions\")\n",
    "    print(\"--------------------------------------------\")\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    all_outputs = []\n",
    "    # Iterate over data.\n",
    "    for inputs, labels in dataloaders['train']:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        all_outputs.append(outputs)\n",
    "\n",
    "    return np.vstack(all_outputs)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Magic Happens here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(data_set, images_path):\n",
    "        \n",
    "    images = []\n",
    "    \n",
    "    \n",
    "    for image_name in data_set['data']:\n",
    "        file = images_path+\"/\"+image_name\n",
    "        img = cv2.imread(file)\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        images.append(img_rgb)\n",
    "\n",
    "   \n",
    "    data_set['images'] = images\n",
    "   \n",
    "    return data_set\n",
    "\n",
    "\n",
    "\n",
    "def train_network(data_set, images_path):\n",
    "    \n",
    "    data_set = load_images(data_set,images_path)\n",
    "    \n",
    "\n",
    "    dataloaders, dataset_sizes, data_stats = make_dataset(data_set,shuffle=True)\n",
    "    model = getModel()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "    trained_model = train_model(model, dataloaders, dataset_sizes, criterion, optimizer, exp_lr_scheduler, num_epochs=10)\n",
    "    \n",
    "  \n",
    "    \n",
    "    #CleanUp\n",
    "    del dataloaders\n",
    "    del model\n",
    "    del criterion\n",
    "    del optimizer\n",
    "    del exp_lr_scheduler\n",
    "    \n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return trained_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = None\n",
    "if RETRAIN:\n",
    "    trained_model = train_network(retrain_prepared_data, RETRAIN_IMAGE_PATH)\n",
    "else:\n",
    "    trained_model = getModel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract and save Tabular Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_save_tabular_data(trained_model=trained_model):\n",
    "    \n",
    "    data_set = load_images(prepared_data,IMAGE_PATH)\n",
    "    dataloaders, dataset_sizes, data_stats = make_dataset(data_set,batch_size=1)\n",
    "    tabular_data = get_predictions(trained_model, dataloaders, dataset_sizes)\n",
    "    \n",
    "    #columns \n",
    "    columns = ['feat_'+str(i+1) for i in range(tabular_data.shape[1])]\n",
    "    \n",
    "    #generating dataframe\n",
    "    tabular_df = pd.DataFrame(data=tabular_data, columns=columns)\n",
    "    \n",
    "    #adding labels\n",
    "    tabular_df['CATEGORY'] = data[CATEGORY_COLUMN]\n",
    "    \n",
    "    #save to tabular directory\n",
    "    tabular_csv_path = os.path.join(TABULAR_PATH,'labels.csv')\n",
    "    tabular_df.to_csv(tabular_csv_path, index=False)\n",
    "    \n",
    "    print(\"--------------------------------------------\")\n",
    "    print(\"Tabular Dataset Saved\")\n",
    "    print(\"--------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_and_save_tabular_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
